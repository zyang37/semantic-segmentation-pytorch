{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Segmentation Demo\n",
    "\n",
    "This is a notebook for running the benchmark semantic segmentation network from the the [ADE20K MIT Scene Parsing Benchchmark](http://sceneparsing.csail.mit.edu/).\n",
    "\n",
    "The code for this notebook is available here\n",
    "https://github.com/CSAILVision/semantic-segmentation-pytorch/tree/master/notebooks\n",
    "\n",
    "It can be run on Colab at this URL https://colab.research.google.com/github/CSAILVision/semantic-segmentation-pytorch/blob/master/notebooks/DemoSegmenter.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "\n",
    "First, download the code and pretrained models if we are on colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "%%bash\n",
    "# Colab-specific setup\n",
    "!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit \n",
    "pip install yacs 2>&1 >> install.log\n",
    "git init 2>&1 >> install.log\n",
    "git remote add origin https://github.com/CSAILVision/semantic-segmentation-pytorch.git 2>> install.log\n",
    "git pull origin master 2>&1 >> install.log\n",
    "DOWNLOAD_ONLY=1 ./demo_test.sh 2>> install.log\n",
    "'''\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and utility functions\n",
    "\n",
    "We need pytorch, numpy, and the code for the segmentation model.  And some utilities for visualizing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "# System libs\n",
    "import os, csv, torch, numpy, scipy.io, PIL.Image, torchvision.transforms\n",
    "# Our libs\n",
    "from mit_semseg.models import ModelBuilder, SegmentationModule\n",
    "from mit_semseg.utils import colorEncode, unique\n",
    "\n",
    "colors = scipy.io.loadmat('data/color150.mat')['colors']\n",
    "names = {}\n",
    "with open('data/object150_info.csv') as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        names[int(row[0])] = row[5].split(\";\")[0]\n",
    "\n",
    "def visualize_result(img, pred, index=None, show=True):\n",
    "    # filter prediction class if requested\n",
    "    if index is not None:\n",
    "        pred = pred.copy()\n",
    "        pred[pred != index] = -1\n",
    "        print(f'{names[index+1]}:')\n",
    "        \n",
    "    # colorize prediction\n",
    "    pred_color = colorEncode(pred, colors).astype(numpy.uint8)\n",
    "\n",
    "    # aggregate images and save\n",
    "    im_vis = numpy.concatenate((img, pred_color), axis=1)\n",
    "    if show==True:\n",
    "        display(PIL.Image.fromarray(im_vis))\n",
    "    else:\n",
    "        return pred_color, im_vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the segmentation model\n",
    "\n",
    "Here we load a pretrained segmentation model.  Like any pytorch model, we can call it like a function, or examine the parameters in all the layers.\n",
    "\n",
    "After loading, we put it on the GPU.  And since we are doing inference, not training, we put the model in eval mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Builders\n",
    "net_encoder = ModelBuilder.build_encoder(\n",
    "    arch='resnet50dilated',\n",
    "    fc_dim=2048,\n",
    "    weights='ckpt/ade20k-resnet50dilated-ppm_deepsup/encoder_epoch_20.pth')\n",
    "net_decoder = ModelBuilder.build_decoder(\n",
    "    arch='ppm_deepsup',\n",
    "    fc_dim=2048,\n",
    "    num_class=150,\n",
    "    weights='ckpt/ade20k-resnet50dilated-ppm_deepsup/decoder_epoch_20.pth',\n",
    "    use_softmax=True)\n",
    "\n",
    "crit = torch.nn.NLLLoss(ignore_index=-1)\n",
    "segmentation_module = SegmentationModule(net_encoder, net_decoder, crit)\n",
    "segmentation_module.eval()\n",
    "segmentation_module.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test data\n",
    "\n",
    "Now we load and normalize a single test image.  Here we use the commonplace convention of normalizing the image to a scale for which the RGB values of a large photo dataset would have zero mean and unit standard deviation.  (These numbers come from the imagenet dataset.)  With this normalization, the limiiting ranges of RGB values are within about (-2.2 to +2.7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_img(path=None, frame=None):\n",
    "    # Load and normalize one image as a singleton tensor batch\n",
    "    pil_to_tensor = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], # These are RGB mean+std values\n",
    "            std=[0.229, 0.224, 0.225])  # across a large photo dataset.\n",
    "    ])\n",
    "    # pil_image = PIL.Image.open('../ADE_val_00001519.jpg').convert('RGB')\n",
    "    if path!=None:\n",
    "        pil_image = PIL.Image.open(path).convert('RGB')\n",
    "    else:\n",
    "        pil_image = PIL.Image.fromarray(frame)\n",
    "    \n",
    "    img_original = numpy.array(pil_image)\n",
    "    img_data = pil_to_tensor(pil_image)\n",
    "    singleton_batch = {'img_data': img_data[None].cuda()}\n",
    "    # singleton_batch = {'img_data': img_data[None]}\n",
    "    output_size = img_data.shape[1:]\n",
    "    return img_original, singleton_batch, output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_original, singleton_batch, output_size = process_img('../ADE_val_00001519.jpg')\n",
    "img_original, singleton_batch, output_size = process_img(\"/home/zyang/Downloads/car_detection_sample1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(PIL.Image.fromarray(img_original))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transparent_overlays function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def transparent_overlays(image, annotation, alpha=0.5):\n",
    "    img1 = image.copy()\n",
    "    img2 = annotation.copy()\n",
    "\n",
    "    # I want to put logo on top-left corner, So I create a ROI\n",
    "    rows,cols,channels = img2.shape\n",
    "    roi = img1[0:rows, 0:cols ]\n",
    "\n",
    "    # Now create a mask of logo and create its inverse mask also\n",
    "    img2gray = cv2.cvtColor(img2,cv2.COLOR_BGR2GRAY)\n",
    "    ret, mask = cv2.threshold(img2gray, 10, 255, cv2.THRESH_BINARY)\n",
    "    mask_inv = cv2.bitwise_not(mask)\n",
    "\n",
    "    # Now black-out the area of logo in ROI\n",
    "    # img1_bg = cv2.bitwise_and(roi,roi,mask = mask_inv)\n",
    "\n",
    "    # Take only region of logo from logo image.\n",
    "    img2_fg = cv2.bitwise_and(img2,img2,mask = mask)\n",
    "\n",
    "    # Put logo in ROI and modify the main image\n",
    "    # dst = cv2.add(img1_bg, img2_fg)\n",
    "    dst = cv2.addWeighted(image.copy(), 1-alpha, img2_fg, alpha, 0)\n",
    "    img1[0:rows, 0:cols ] = dst\n",
    "    return dst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Model\n",
    "\n",
    "Finally we just pass the test image to the segmentation model.\n",
    "\n",
    "The segmentation model is coded as a function that takes a dictionary as input, because it wants to know both the input batch image data as well as the desired output segmentation resolution.  We ask for full resolution output.\n",
    "\n",
    "Then we use the previously-defined visualize_result function to render the segmentation map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def predict_img(segmentation_module, singleton_batch, output_size):\n",
    "    # Run the segmentation at the highest resolution.\n",
    "    with torch.no_grad():\n",
    "        scores = segmentation_module(singleton_batch, segSize=output_size)\n",
    "    \n",
    "    # Get the predicted scores for each pixel\n",
    "    _, pred = torch.max(scores, dim=1)\n",
    "    pred = pred.cpu()[0].numpy()\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict_img(segmentation_module, singleton_batch, output_size)\n",
    "pred_color, im_vis = visualize_result(img_original, pred, show=False)\n",
    "display(PIL.Image.fromarray(im_vis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst = transparent_overlays(img_original, pred_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(PIL.Image.fromarray(dst))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append color palette\n",
    "\n",
    "To see which colors are which, here we visualize individual classes, one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Top classes in answer\n",
    "predicted_classes = numpy.bincount(pred.flatten()).argsort()[::-1]\n",
    "for c in predicted_classes[:15]:\n",
    "    # visualize_result(img_original, pred, c)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.int32(pred)\n",
    "pixs = pred.size\n",
    "    \n",
    "uniques, counts = np.unique(pred, return_counts=True)\n",
    "#print(\"Predictions in [{}]:\".format(info))\n",
    "for idx in np.argsort(counts)[::-1]:\n",
    "    name = names[uniques[idx] + 1]\n",
    "    ratio = counts[idx] / pixs * 100\n",
    "    if ratio > 0.1:\n",
    "        print(\"{}  {}: {:.2f}% {}\".format(uniques[idx]+1, name, ratio, colors[uniques[idx]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color_palette(pred, bar_height):\n",
    "\n",
    "    pred = np.int32(pred)\n",
    "    pixs = pred.size\n",
    "\n",
    "    top_left_y = 0\n",
    "    bottom_right_y = 30\n",
    "    uniques, counts = np.unique(pred, return_counts=True)\n",
    "\n",
    "    # Create a black image\n",
    "    # bar_height = im_vis.shape[0]\n",
    "    img = np.zeros((bar_height,250,3), np.uint8)\n",
    "\n",
    "    for idx in np.argsort(counts)[::-1]:\n",
    "        color_index = uniques[idx]\n",
    "        name = names[color_index + 1]\n",
    "        ratio = counts[idx] / pixs * 100\n",
    "        if ratio > 0.1:\n",
    "            print(\"{}  {}: {:.2f}% {}\".format(color_index+1, name, ratio, colors[color_index]))\n",
    "            img = cv2.rectangle(img, (0,top_left_y), (250,bottom_right_y), \n",
    "                       (int(colors[color_index][0]),int(colors[color_index][1]),int(colors[color_index][2])), -1)\n",
    "            img = cv2.putText(img, \"{}: {:.3f}%\".format(name, ratio), (0,top_left_y+20), 5, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "            top_left_y+=30\n",
    "            bottom_right_y+=30\n",
    "            \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = get_color_palette(pred, im_vis.shape[0])\n",
    "display(PIL.Image.fromarray(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "# display(PIL.Image.fromarray(img))\n",
    "pred_color_palette = numpy.concatenate((pred_color, img), axis=1)\n",
    "pred_color_palette_dst = numpy.concatenate((dst, img), axis=1)\n",
    "pred_color_palette_all = numpy.concatenate((im_vis, img), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(PIL.Image.fromarray(pred_color))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pred_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"pred_color.png\",cv2.cvtColor(pred_color, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(PIL.Image.fromarray(pred_color_palette))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(PIL.Image.fromarray(pred_color_palette_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(PIL.Image.fromarray(pred_color_palette_dst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
