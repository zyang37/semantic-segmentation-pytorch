{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "medical-coalition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System libs\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "from distutils.version import LooseVersion\n",
    "# Numerical libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.io import loadmat\n",
    "# Our libs\n",
    "from mit_semseg.config import cfg\n",
    "from mit_semseg.dataset import ValDataset\n",
    "from mit_semseg.models import ModelBuilder, SegmentationModule\n",
    "from mit_semseg.utils import AverageMeter, colorEncode, accuracy, intersectionAndUnion, setup_logger\n",
    "from mit_semseg.lib.nn import user_scattered_collate, async_copy_to\n",
    "from mit_semseg.lib.utils import as_numpy\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys, csv, torch, numpy, scipy.io, PIL.Image, torchvision.transforms\n",
    "\n",
    "sys.path.insert(1, '/home/zyang/Documents/Noisey-image')\n",
    "\n",
    "from noise_video_gen import *\n",
    "\n",
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "rational-belle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import yaml\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "##################### model stuff #####################\n",
    "# System libs\n",
    "import csv, torch, scipy.io, PIL.Image, torchvision.transforms\n",
    "# Our libs\n",
    "from mit_semseg.config import cfg\n",
    "from mit_semseg.utils import colorEncode\n",
    "from mit_semseg.dataset import ValDataset\n",
    "from mit_semseg.lib.utils import as_numpy\n",
    "from mit_semseg.models import ModelBuilder, SegmentationModule\n",
    "from mit_semseg.lib.nn import user_scattered_collate, async_copy_to\n",
    "from mit_semseg.utils import AverageMeter, colorEncode, accuracy, intersectionAndUnion, setup_logger\n",
    "\n",
    "# pass in mode config(yaml file)\n",
    "# return a dict for the file \n",
    "# return decoder and encoder weights path\n",
    "def parse_model_config(path):\n",
    "    with open(path) as file:\n",
    "        data = yaml.load(file, Loader=yaml.FullLoader)\n",
    "    \n",
    "    encoder_path = None\n",
    "    decoder_path = None\n",
    "\n",
    "    for p in os.listdir(data['DIR']):\n",
    "        if \"encoder\" in p.lower():\n",
    "            encoder_path = \"{}/{}\".format(data['DIR'], p)\n",
    "            continue\n",
    "        if \"decoder\" in p.lower():\n",
    "            decoder_path = \"{}/{}\".format(data['DIR'], p)\n",
    "            continue\n",
    "\n",
    "    if encoder_path==None or decoder_path==None:\n",
    "        raise(\"model weights not found\")\n",
    "        \n",
    "    return data, encoder_path, decoder_path\n",
    "\n",
    "def visualize_result(img, pred, index=None):\n",
    "    # filter prediction class if requested\n",
    "    if index is not None:\n",
    "        pred = pred.copy()\n",
    "        pred[pred != index] = -1\n",
    "        print(f'{names[index+1]}:')\n",
    "\n",
    "    # colorize prediction\n",
    "    pred_color = colorEncode(pred, colors).astype(numpy.uint8)\n",
    "\n",
    "    # aggregate images and save\n",
    "    im_vis = numpy.concatenate((img, pred_color), axis=1)\n",
    "    #if show==True:\n",
    "        #display(PIL.Image.fromarray(im_vis))\n",
    "    #else:\n",
    "    return pred_color, im_vis\n",
    "\n",
    "def process_img(path=None, frame=None):\n",
    "    # Load and normalize one image as a singleton tensor batch\n",
    "    pil_to_tensor = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], # These are RGB mean+std values\n",
    "            std=[0.229, 0.224, 0.225])  # across a large photo dataset.\n",
    "    ])\n",
    "    # pil_image = PIL.Image.open('../ADE_val_00001519.jpg').convert('RGB')\n",
    "    if path!=None:\n",
    "        pil_image = PIL.Image.open(path).convert('RGB')\n",
    "    else:\n",
    "        pil_image = PIL.Image.fromarray(frame)\n",
    "\n",
    "    img_original = numpy.array(pil_image)\n",
    "    img_data = pil_to_tensor(pil_image)\n",
    "    singleton_batch = {'img_data': img_data[None].cuda()}\n",
    "    output_size = img_data.shape[1:]\n",
    "    return (img_original, singleton_batch, output_size)\n",
    "\n",
    "def predict_img(segmentation_module, singleton_batch, output_size):\n",
    "    # Run the segmentation at the highest resolution.\n",
    "    with torch.no_grad():\n",
    "        scores = segmentation_module(singleton_batch, segSize=output_size)\n",
    "\n",
    "    # Get the predicted scores for each pixel\n",
    "    _, pred = torch.max(scores, dim=1)\n",
    "    pred = pred.cpu()[0].numpy()\n",
    "    return pred\n",
    "\n",
    "\n",
    "def get_color_palette(pred, bar_height):\n",
    "\n",
    "    pred = np.int32(pred)\n",
    "    pixs = pred.size\n",
    "\n",
    "    top_left_y = 0\n",
    "    bottom_right_y = 30\n",
    "    uniques, counts = np.unique(pred, return_counts=True)\n",
    "\n",
    "    # Create a black image\n",
    "    # bar_height = im_vis.shape[0]\n",
    "    img = np.zeros((bar_height,250,3), np.uint8)\n",
    "\n",
    "    for idx in np.argsort(counts)[::-1]:\n",
    "        color_index = uniques[idx]\n",
    "        name = names[color_index + 1]\n",
    "        ratio = counts[idx] / pixs * 100\n",
    "        if ratio > 0.1:\n",
    "            print(\"{}  {}: {:.2f}% {}\".format(color_index+1, name, ratio, colors[color_index]))\n",
    "            img = cv2.rectangle(img, (0,top_left_y), (250,bottom_right_y), \n",
    "                       (int(colors[color_index][0]),int(colors[color_index][1]),int(colors[color_index][2])), -1)\n",
    "            img = cv2.putText(img, \"{}: {:.3f}%\".format(name, ratio), (0,top_left_y+20), 5, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "            top_left_y+=30\n",
    "            bottom_right_y+=30\n",
    "            \n",
    "    return img\n",
    "\n",
    "\n",
    "def transparent_overlays(image, annotation, alpha=0.5):\n",
    "    img1 = image.copy()\n",
    "    img2 = annotation.copy()\n",
    "\n",
    "    # I want to put logo on top-left corner, So I create a ROI\n",
    "    rows,cols,channels = img2.shape\n",
    "    roi = img1[0:rows, 0:cols ]\n",
    "\n",
    "    # Now create a mask of logo and create its inverse mask also\n",
    "    img2gray = cv2.cvtColor(img2,cv2.COLOR_BGR2GRAY)\n",
    "    ret, mask = cv2.threshold(img2gray, 10, 255, cv2.THRESH_BINARY)\n",
    "    mask_inv = cv2.bitwise_not(mask)\n",
    "\n",
    "    # Now black-out the area of logo in ROI\n",
    "    # img1_bg = cv2.bitwise_and(roi,roi,mask = mask_inv)\n",
    "\n",
    "    # Take only region of logo from logo image.\n",
    "    img2_fg = cv2.bitwise_and(img2,img2,mask = mask)\n",
    "\n",
    "    # Put logo in ROI and modify the main image\n",
    "    # dst = cv2.add(img1_bg, img2_fg)\n",
    "    dst = cv2.addWeighted(image.copy(), 1-alpha, img2_fg, alpha, 0)\n",
    "    img1[0:rows, 0:cols ] = dst\n",
    "    return dst\n",
    "\n",
    "\n",
    "def evaluate(segmentation_module, loader, cfg, gpu):\n",
    "    acc_meter = AverageMeter()\n",
    "    intersection_meter = AverageMeter()\n",
    "    union_meter = AverageMeter()\n",
    "    time_meter = AverageMeter()\n",
    "\n",
    "    segmentation_module.eval()\n",
    "\n",
    "    pbar = tqdm(total=len(loader), leave=False)\n",
    "    for batch_data in loader:\n",
    "        # process data\n",
    "        batch_data = batch_data[0]\n",
    "        seg_label = as_numpy(batch_data['seg_label'][0])\n",
    "        img_resized_list = batch_data['img_data']\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        tic = time.perf_counter()\n",
    "        with torch.no_grad():\n",
    "            segSize = (seg_label.shape[0], seg_label.shape[1])\n",
    "            scores = torch.zeros(1, cfg.DATASET.num_class, segSize[0], segSize[1])\n",
    "            scores = async_copy_to(scores, gpu)\n",
    "\n",
    "            for img in img_resized_list:\n",
    "                feed_dict = batch_data.copy()\n",
    "                feed_dict['img_data'] = img\n",
    "                del feed_dict['img_ori']\n",
    "                del feed_dict['info']\n",
    "                feed_dict = async_copy_to(feed_dict, gpu)\n",
    "\n",
    "                # forward pass\n",
    "                scores_tmp = segmentation_module(feed_dict, segSize=segSize)\n",
    "                scores = scores + scores_tmp / len(cfg.DATASET.imgSizes)\n",
    "\n",
    "            _, pred = torch.max(scores, dim=1)\n",
    "            pred = as_numpy(pred.squeeze(0).cpu())\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        time_meter.update(time.perf_counter() - tic)\n",
    "\n",
    "        # calculate accuracy\n",
    "        acc, pix = accuracy(pred, seg_label)\n",
    "        intersection, union = intersectionAndUnion(pred, seg_label, cfg.DATASET.num_class)\n",
    "        acc_meter.update(acc, pix)\n",
    "        intersection_meter.update(intersection)\n",
    "        union_meter.update(union)\n",
    "\n",
    "        # visualization\n",
    "        if cfg.VAL.visualize:\n",
    "            visualize_result(\n",
    "                (batch_data['img_ori'], seg_label, batch_data['info']),\n",
    "                pred,\n",
    "                os.path.join(cfg.DIR, 'result')\n",
    "            )\n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "    # summary\n",
    "    iou = intersection_meter.sum / (union_meter.sum + 1e-10)\n",
    "    #for i, _iou in enumerate(iou):\n",
    "        #print('class [{}], IoU: {:.4f}'.format(i, _iou))\n",
    "\n",
    "    #print('[Eval Summary]:')\n",
    "    #print('Mean IoU: {:.4f}, Accuracy: {:.2f}%, Inference Time: {:.4f}s'\n",
    "    #      .format(iou.mean(), acc_meter.average()*100, time_meter.average()))\n",
    "    \n",
    "    return pred, acc_meter.average()*100\n",
    "\n",
    "\n",
    "# create a tmp_obgt for 1 img\n",
    "def create_tmp_obgt(img_path, anno_path, width, height, tmp_path=\"tmp_results/tmp_eval.odgt\"):\n",
    "    eval_img = \"{\\\"fpath_img\\\": \" + \"\\\"{}\\\", \".format(img_path) + \"\\\"fpath_segm\\\": \" + \"\\\"{}\\\", \".format(anno_path) + \\\n",
    "                \"\\\"width\\\": \" + str(width) + \", \\\"height\\\": \" + str(height) + \"}\"\n",
    "    f = open(tmp_path, \"w\")\n",
    "    f.write(\"{}\".format(eval_img))\n",
    "    f.close()\n",
    "    \n",
    "    \n",
    "def setup_oneImg_loader(img_path, anno_path, tmp_path=\"tmp_results/tmp_eval.odgt\", update=0):\n",
    "    img = PIL.Image.open(img_path).convert('RGB')\n",
    "    anno_rgb = PIL.Image.open(anno_path).convert('RGB')\n",
    "    anno = PIL.Image.open(anno_path)\n",
    "    anno = np.array(anno)\n",
    "    anno[np.where(anno!=0)]-=1\n",
    "\n",
    "    width = img.size[0]\n",
    "    height = img.size[1]\n",
    "    # im_vis = numpy.concatenate((img, anno_rgb), axis=1)\n",
    "    # display(PIL.Image.fromarray(im_vis))\n",
    "    if update==0:\n",
    "        create_tmp_obgt(img_path, anno_path, width, height, tmp_path=tmp_path)\n",
    "    \n",
    "    # Dataset and Loader\n",
    "    dataset_val = ValDataset(\"\", tmp_path, cfg.DATASET)\n",
    "    loader_val = torch.utils.data.DataLoader(\n",
    "        dataset_val,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        collate_fn=user_scattered_collate,\n",
    "        num_workers=1,\n",
    "        drop_last=True)\n",
    "    \n",
    "    return loader_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "trained-accountability",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"/home/zyang/Documents/fork_sseg_mit/notebooks/data/ADEChallengeData2016/images/training/ADE_train_00000001.jpg\"\n",
    "anno_path = \"/home/zyang/Documents/fork_sseg_mit/notebooks/data/ADEChallengeData2016/annotations/training/ADE_train_00000001.png\"\n",
    "\n",
    "tmp_img_path = '../tmp_results/tmp.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "composite-frost",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "anno = PIL.Image.open(anno_path)\n",
    "anno = np.array(anno)\n",
    "anno[np.where(anno!=0)]-=1\n",
    "\n",
    "'''\n",
    "img = PIL.Image.open(img_path).convert('RGB')\n",
    "anno_rgb = PIL.Image.open(anno_path).convert('RGB')\n",
    "\n",
    "width = img.size[0]\n",
    "height = img.size[1]\n",
    "\n",
    "im_vis = numpy.concatenate((img, anno_rgb), axis=1)\n",
    "display(PIL.Image.fromarray(im_vis))\n",
    "'''\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "lined-helicopter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# Dataset and Loader\n",
    "dataset_val = ValDataset(\"\", \"tmp_eval.odgt\", cfg.DATASET)\n",
    "loader_val = torch.utils.data.DataLoader(\n",
    "    dataset_val,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=user_scattered_collate,\n",
    "    num_workers=1,\n",
    "    drop_last=True)\n",
    "'''\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "brief-netscape",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing config/ade20k-resnet50dilated-ppm_deepsup.yaml\n",
      "Loading weights for net_encoder\n",
      "Loading weights for net_decoder\n",
      "\n",
      "Evaluation on\n",
      "Images: /home/zyang/Documents/fork_sseg_mit/notebooks/data/ADEChallengeData2016/images/training/ADE_train_00000001.jpg\n",
      "Annotation: /home/zyang/Documents/fork_sseg_mit/notebooks/data/ADEChallengeData2016/images/training/ADE_train_00000001.jpg\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# colors\n",
    "colors = scipy.io.loadmat('data/color150.mat')['colors']\n",
    "names = {}\n",
    "with open('data/object150_info.csv') as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        names[int(row[0])] = row[5].split(\";\")[0]\n",
    "\n",
    "# parse cfg\n",
    "cfg.merge_from_file(\"config/ade20k-resnet50dilated-ppm_deepsup.yaml\")\n",
    "# cfg.merge_from_list(opts)\n",
    "\n",
    "# Network Builders\n",
    "print(\"parsing {}\".format(\"config/ade20k-resnet50dilated-ppm_deepsup.yaml\"))\n",
    "model_config, encoder_path, decoder_path = parse_model_config(\"config/ade20k-resnet50dilated-ppm_deepsup.yaml\")\n",
    "net_encoder = ModelBuilder.build_encoder(\n",
    "    arch = model_config[\"MODEL\"]['arch_encoder'],\n",
    "    fc_dim = model_config['MODEL']['fc_dim'],\n",
    "    weights = encoder_path)\n",
    "net_decoder = ModelBuilder.build_decoder(\n",
    "    arch = model_config[\"MODEL\"]['arch_decoder'],\n",
    "    fc_dim = model_config['MODEL']['fc_dim'],\n",
    "    num_class = model_config['DATASET']['num_class'],\n",
    "    weights = decoder_path,\n",
    "    use_softmax=True)\n",
    "\n",
    "crit = torch.nn.NLLLoss(ignore_index=-1)\n",
    "segmentation_module = SegmentationModule(net_encoder, net_decoder, crit)\n",
    "# segmentation_module.eval()\n",
    "segmentation_module.cuda()\n",
    "\n",
    "# predict on 1 img\n",
    "print(\"\\nEvaluation on\\nImages: {}\\nAnnotation: {}\\n\".format(img_path, img_path))\n",
    "#img_path = args.img\n",
    "#anno_path = args.anno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dried-cargo",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../tmp_results/tmp.jpg'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "billion-coalition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/zyang/Documents/fork_sseg_mit/notebooks/data/ADEChallengeData2016/images/training/ADE_train_00000001.jpg'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "assured-shoulder",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# samples: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.16it/s]/home/zyang/Documents/Noisey-image/noise_video_gen.py:15: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  out[coords] = 1\n",
      "/home/zyang/Documents/Noisey-image/noise_video_gen.py:20: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  out[coords] = 0\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# samples: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.62it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# samples: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# samples: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.62it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# samples: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "all_amount = []\n",
    "all_acc = []\n",
    "pred_num_class = []\n",
    "\n",
    "# noise\n",
    "copyfile(img_path, tmp_img_path)\n",
    "\n",
    "img = cv2.imread(tmp_img_path)\n",
    "org_img = img.copy()\n",
    "\n",
    "amount = 0.0\n",
    "while(True):\n",
    "    loader_val = setup_oneImg_loader(tmp_img_path, anno_path, tmp_path=\"../tmp_results/tmp_eval.odgt\", update=0)\n",
    "    segmentation_module.cuda()\n",
    "    pred, acc = evaluate(segmentation_module, loader_val, cfg, 0)\n",
    "    \n",
    "    all_amount.append(amount)\n",
    "    all_acc.append(acc)\n",
    "    pred_num_class.append(len(np.unique(pred)))\n",
    "    \n",
    "    amount+=0.0001\n",
    "    frame = saltAndPapper_noise(org_img, s_vs_p=0.5, amount=amount)\n",
    "    \n",
    "    # overwrite tmp.jpg\n",
    "    cv2.imwrite(tmp_img_path, frame)\n",
    "    \n",
    "    count+=1\n",
    "    if count==5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "unsigned-nursing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[90.93313963631054,\n",
       " 90.46718833757835,\n",
       " 89.37510506900453,\n",
       " 78.77843999885809,\n",
       " 63.714161557273286]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "emotional-cardiff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13, 12, 12, 12, 12]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_num_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "touched-atlas",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amount</th>\n",
       "      <th>PixelAcc</th>\n",
       "      <th>num_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>90.933140</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001</td>\n",
       "      <td>90.467188</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002</td>\n",
       "      <td>89.375105</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.003</td>\n",
       "      <td>78.778440</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.004</td>\n",
       "      <td>63.714162</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   amount   PixelAcc  num_class\n",
       "0   0.000  90.933140         13\n",
       "1   0.001  90.467188         12\n",
       "2   0.002  89.375105         12\n",
       "3   0.003  78.778440         12\n",
       "4   0.004  63.714162         12"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_name = 'tmp'\n",
    "df = pd.DataFrame()\n",
    "df['amount'] = all_amount\n",
    "df['PixelAcc'] = all_acc\n",
    "df['num_class'] = pred_num_class\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "piano-extent",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amount</th>\n",
       "      <th>PixelAcc</th>\n",
       "      <th>num_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>90.933140</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>90.716182</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>90.422782</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0015</td>\n",
       "      <td>90.199480</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0020</td>\n",
       "      <td>88.616705</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0025</td>\n",
       "      <td>87.015533</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0030</td>\n",
       "      <td>74.389807</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0035</td>\n",
       "      <td>67.150909</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0040</td>\n",
       "      <td>65.777479</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0045</td>\n",
       "      <td>68.066001</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0050</td>\n",
       "      <td>61.406608</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0055</td>\n",
       "      <td>59.869191</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0060</td>\n",
       "      <td>56.243081</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0065</td>\n",
       "      <td>54.164856</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0070</td>\n",
       "      <td>51.320618</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0075</td>\n",
       "      <td>58.291174</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0080</td>\n",
       "      <td>48.755824</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0085</td>\n",
       "      <td>53.780422</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0090</td>\n",
       "      <td>45.992153</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0095</td>\n",
       "      <td>49.606844</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>47.746210</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0105</td>\n",
       "      <td>52.341651</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0110</td>\n",
       "      <td>48.506196</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0115</td>\n",
       "      <td>45.891604</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0120</td>\n",
       "      <td>46.832705</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0125</td>\n",
       "      <td>54.971786</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0130</td>\n",
       "      <td>46.460324</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0135</td>\n",
       "      <td>48.288287</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0140</td>\n",
       "      <td>55.062502</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0145</td>\n",
       "      <td>50.564439</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.0150</td>\n",
       "      <td>50.529231</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.0155</td>\n",
       "      <td>48.039928</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.0160</td>\n",
       "      <td>44.786516</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.0165</td>\n",
       "      <td>47.481992</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.0170</td>\n",
       "      <td>45.933790</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.0175</td>\n",
       "      <td>49.955118</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.0180</td>\n",
       "      <td>43.398177</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.0185</td>\n",
       "      <td>48.776759</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.0190</td>\n",
       "      <td>50.281823</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.0195</td>\n",
       "      <td>42.153526</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>39.732102</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.0205</td>\n",
       "      <td>42.214744</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.0210</td>\n",
       "      <td>43.437509</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.0215</td>\n",
       "      <td>45.477989</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.0220</td>\n",
       "      <td>48.456715</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.0225</td>\n",
       "      <td>41.379584</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.0230</td>\n",
       "      <td>44.226676</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.0235</td>\n",
       "      <td>45.176341</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.0240</td>\n",
       "      <td>43.176462</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.0245</td>\n",
       "      <td>41.325027</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    amount   PixelAcc  num_class\n",
       "0   0.0000  90.933140         13\n",
       "1   0.0005  90.716182         13\n",
       "2   0.0010  90.422782         13\n",
       "3   0.0015  90.199480         12\n",
       "4   0.0020  88.616705         12\n",
       "5   0.0025  87.015533         11\n",
       "6   0.0030  74.389807         12\n",
       "7   0.0035  67.150909         11\n",
       "8   0.0040  65.777479         11\n",
       "9   0.0045  68.066001         11\n",
       "10  0.0050  61.406608         11\n",
       "11  0.0055  59.869191         12\n",
       "12  0.0060  56.243081         11\n",
       "13  0.0065  54.164856         11\n",
       "14  0.0070  51.320618          9\n",
       "15  0.0075  58.291174         10\n",
       "16  0.0080  48.755824         10\n",
       "17  0.0085  53.780422         12\n",
       "18  0.0090  45.992153         11\n",
       "19  0.0095  49.606844         12\n",
       "20  0.0100  47.746210         11\n",
       "21  0.0105  52.341651         11\n",
       "22  0.0110  48.506196         10\n",
       "23  0.0115  45.891604         11\n",
       "24  0.0120  46.832705         12\n",
       "25  0.0125  54.971786         12\n",
       "26  0.0130  46.460324         11\n",
       "27  0.0135  48.288287         12\n",
       "28  0.0140  55.062502         12\n",
       "29  0.0145  50.564439         12\n",
       "30  0.0150  50.529231         12\n",
       "31  0.0155  48.039928         12\n",
       "32  0.0160  44.786516         12\n",
       "33  0.0165  47.481992         12\n",
       "34  0.0170  45.933790         12\n",
       "35  0.0175  49.955118         13\n",
       "36  0.0180  43.398177         12\n",
       "37  0.0185  48.776759         12\n",
       "38  0.0190  50.281823         14\n",
       "39  0.0195  42.153526         12\n",
       "40  0.0200  39.732102         12\n",
       "41  0.0205  42.214744         12\n",
       "42  0.0210  43.437509         13\n",
       "43  0.0215  45.477989         12\n",
       "44  0.0220  48.456715         12\n",
       "45  0.0225  41.379584         13\n",
       "46  0.0230  44.226676         13\n",
       "47  0.0235  45.176341         12\n",
       "48  0.0240  43.176462         12\n",
       "49  0.0245  41.325027         12"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../tmp_results/s_p50.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-designer",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(PIL.Image.fromarray(pred.astype(np.uint8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-freeze",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "published-examination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # classes for pred\n",
    "len(np.unique(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "double-subscription",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(anno))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-bikini",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System libs\n",
    "import os, csv, torch, numpy, scipy.io, PIL.Image, torchvision.transforms\n",
    "# Our libs\n",
    "from mit_semseg.models import ModelBuilder, SegmentationModule\n",
    "from mit_semseg.utils import colorEncode\n",
    "\n",
    "colors = scipy.io.loadmat('data/color150.mat')['colors']\n",
    "names = {}\n",
    "with open('data/object150_info.csv') as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        names[int(row[0])] = row[5].split(\";\")[0]\n",
    "\n",
    "def visualize_result(img, pred, index=None, show=True):\n",
    "    # filter prediction class if requested\n",
    "    if index is not None:\n",
    "        pred = pred.copy()\n",
    "        pred[pred != index] = -1\n",
    "        print(f'{names[index+1]}:')\n",
    "        \n",
    "    # colorize prediction\n",
    "    pred_color = colorEncode(pred, colors).astype(numpy.uint8)\n",
    "\n",
    "    # aggregate images and save\n",
    "    im_vis = numpy.concatenate((img, pred_color), axis=1)\n",
    "    if show==True:\n",
    "        display(PIL.Image.fromarray(im_vis))\n",
    "    else:\n",
    "        return pred_color, im_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valid-removal",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_result(img, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respiratory-graphic",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_result(img, anno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-activation",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(anno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrow-version",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-lawrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.unique(np.append(np.unique(anno), np.unique(pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-spectacular",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-pathology",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
